{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "gdpr_article_5_text = \"\"\"\n",
        "                      1. Personal data shall be:\n",
        "                      a) processed lawfully, fairly and in a transparent manner in relation to the data subject (‘lawfulness, fairness and transparency’);\n",
        "                      b) collected for specified, explicit and legitimate purposes and not further processed in a manner that is incompatible with those purposes; further processing for archiving purposes in the public interest, scientific or historical research purposes or statistical purposes shall, in accordance with Article 89(1), not be considered to be incompatible with the initial purposes (‘purpose limitation’);\n",
        "                      c) adequate, relevant and limited to what is necessary in relation to the purposes for which they are processed (‘data minimisation’);\n",
        "                      d) accurate and, where necessary, kept up to date; every reasonable step must be taken to ensure that personal data that are inaccurate, having regard to the purposes for which they are processed, are erased or rectified without delay (‘accuracy’);\n",
        "                      e) kept in a form which permits identification of data subjects for no longer than is necessary for the purposes for which the personal data are processed; personal data may be stored for longer periods insofar as the personal data will be processed solely for archiving purposes in the public interest, scientific or historical research purposes or statistical purposes in accordance with Article 89(1) subject to implementation of the appropriate technical and organisational measures required by this Regulation in order to safeguard the rights and freedoms of the data subject (‘storage limitation’);\n",
        "                      f) processed in a manner that ensures appropriate security of the personal data, including protection against unauthorised or unlawful processing and against accidental loss, destruction or damage, using appropriate technical or organisational measures (‘integrity and confidentiality’).\n",
        "                      2. The controller shall be responsible for, and be able to demonstrate compliance with, paragraph 1 (‘accountability’).\n",
        "                      \"\"\""
      ],
      "metadata": {
        "id": "gUaWrohTdOKg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/acl-coling-2014-corpus.zip -d /content/acl_corpus"
      ],
      "metadata": {
        "id": "22l8rimTMLrm"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import spacy\n",
        "import os\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Lowercase and replace newline characters\n",
        "    text = text.lower().replace('\\n', ' ')\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    # Use spaCy for further processing\n",
        "    doc = nlp(text)\n",
        "    clean_text = ' '.join([token.lemma_ for token in doc if not token.is_punct])\n",
        "\n",
        "    return clean_text\n",
        "\n",
        "preprocessed_policies = []\n",
        "directory = './acl_corpus/corpus/'\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith('.xml'):\n",
        "        path = os.path.join(directory, filename)\n",
        "        tree = ET.parse(path)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        # Extracting text from each SECTION/SUBTEXT\n",
        "        for section in root.findall('.//SECTION/SUBTEXT'):\n",
        "            if section.text:\n",
        "                processed_text = preprocess_text(section.text)\n",
        "                preprocessed_policies.append(processed_text)\n"
      ],
      "metadata": {
        "id": "baJhs6XB-XQ5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(preprocessed_policies))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzYqvKKCrj2z",
        "outputId": "ae65a228-8783-451d-ecd7-a9eba54f551a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "SJTjwOtP-lER",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bae6982-c51c-4bc7-c299-09c04bc41248"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.7.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.40.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/saved_model.zip -d /content/saved_model"
      ],
      "metadata": {
        "id": "8z4K5zdJGdXh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f65c17f-913a-4a92-b59a-c2661c7037e0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/saved_model.zip\n",
            "   creating: /content/saved_model/saved_model/\n",
            "  inflating: /content/saved_model/saved_model/vocab.txt  \n",
            "  inflating: /content/saved_model/saved_model/tokenizer_config.json  \n",
            "  inflating: /content/saved_model/saved_model/sentence_bert_config.json  \n",
            "  inflating: /content/saved_model/saved_model/tokenizer.json  \n",
            "   creating: /content/saved_model/saved_model/2_Normalize/\n",
            "  inflating: /content/saved_model/saved_model/config.json  \n",
            "  inflating: /content/saved_model/saved_model/modules.json  \n",
            "  inflating: /content/saved_model/saved_model/config_sentence_transformers.json  \n",
            "   creating: /content/saved_model/saved_model/1_Pooling/\n",
            "  inflating: /content/saved_model/saved_model/1_Pooling/config.json  \n",
            "  inflating: /content/saved_model/saved_model/README.md  \n",
            "  inflating: /content/saved_model/saved_model/model.safetensors  \n",
            "  inflating: /content/saved_model/saved_model/special_tokens_map.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "sbert_model = SentenceTransformer('./saved_model/saved_model')\n"
      ],
      "metadata": {
        "id": "eFAly2J0-aXd"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy_embeddings = sbert_model.encode(preprocessed_policies)\n",
        "gdpr_embeddings = sbert_model.encode([gdpr_article_5_text])\n"
      ],
      "metadata": {
        "id": "tXejyz8CdGLM"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Calculate semantic similarity (higher values mean more similarity)\n",
        "similarities = cosine_similarity(policy_embeddings, gdpr_embeddings)\n",
        "similar_policies_indices = np.where(similarities > 0.9)[0]\n",
        "\n",
        "print(f\"Number of policies potentially aligning with GDPR Article 5: {len(similar_policies_indices)}\")\n"
      ],
      "metadata": {
        "id": "uTWs3JG8kdms",
        "outputId": "e8237bf4-9428-4787-c74f-6681ddac9ffe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of policies potentially aligning with GDPR Article 5: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Calculate semantic similarity (higher values mean more similarity)\n",
        "similarities = cosine_similarity(policy_embeddings, gdpr_embeddings)\n",
        "similar_policies_indices = np.where(similarities > 0.6)[0]\n",
        "\n",
        "print(f\"Number of policies potentially aligning with GDPR Article 5: {len(similar_policies_indices)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ak3OFt1m-a_5",
        "outputId": "83e1d30f-39f3-4940-b3c1-96d27e905e67"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of policies potentially aligning with GDPR Article 5: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Calculate semantic similarity (higher values mean more similarity)\n",
        "similarities = cosine_similarity(policy_embeddings, gdpr_embeddings)\n",
        "similar_policies_indices = np.where(similarities > 0.5)[0]\n",
        "\n",
        "print(f\"Number of policies potentially aligning with GDPR Article 5: {len(similar_policies_indices)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dqBVrTDulvc",
        "outputId": "2266a89d-9087-401d-878e-732d36924248"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of policies potentially aligning with GDPR Article 5: 233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Embedding Models:** Other than *SBERT*, I can try other models like *GPT-3* for context-aware embeddings, *FastText* for handling out-of-vocabulary words, or domain-specific models that might be trained on legal or policy-related corpora.\n",
        "\n",
        "- **Similarity Measures:** Beyond *cosine similarity*, explore other measures like *Euclidean distance* or *Manhattan distance* for comparing embeddings.\n",
        "\n",
        "- **Clustering Techniques:** Apply unsupervised clustering (e.g., *K-means, DBSCAN*) to group policies based on their embeddings and analyze clusters for GDPR compliance themes.\n",
        "\n",
        "- **Dimensionality Reduction:** Use techniques like *PCA* or *t-SNE* to reduce the dimensionality of your embeddings before applying similarity measures or clustering, which might reveal different patterns.\n",
        "\n",
        "- **Threshold Tuning:** Experiment with different *thresholds* for considering a policy potentially compliant based on similarity scores.\n",
        "\n",
        "- **Text Preprocessing Variations:** Test the impact of different *preprocessing steps*, such as including/excluding stopwords, using stemming versus lemmatization, or experimenting with n-grams.\n",
        "\n",
        "- **Segmentation Strategies:** Instead of analyzing entire documents, try segmenting policies into smaller *sections or paragraphs* and assess their individual compliance."
      ],
      "metadata": {
        "id": "X5CuRH3npCUm"
      }
    }
  ]
}